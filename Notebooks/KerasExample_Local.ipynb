{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Keras in DSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Update Scikit-Learn\n",
    "!pip install -U scikit-learn\n",
    "\n",
    "## Check version of sklearn\n",
    "!pip freeze | grep scikit-learn\n",
    "\n",
    "## To install Theano\n",
    "!pip install Theano\n",
    "\n",
    "## To install TensorFlow\n",
    "!pip install TensorFlow\n",
    "\n",
    "## To install Keras\n",
    "!pip install keras\n",
    "\n",
    "## Version of Keras\n",
    "!python -c \"import keras; print(keras.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Your First Neural Network With Keras\n",
    "\n",
    "After loading the [Pima Indians data set](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes) we define a simple multi-layer perceptron neural network and test its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "## Set random seed for reproducibility\n",
    "numpy.random.seed(206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access 'pima-indians-diabetes.csv' data file from the project.\n",
    "dbtsDF = ProjectUtil.load_dataframe_from_file(pc, \"pima-indians-diabetes.csv\")\n",
    "dbtsDF = dbtsDF.toPandas()\n",
    "\n",
    "dbtsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split into input and output variables\n",
    "X = dbtsDF.loc[:, '_c0':'_c7'].values\n",
    "Y = dbtsDF.loc[:, '_c8'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model definition\n",
    "mlpMod = Sequential()\n",
    "mlpMod.add(Dense(24, input_dim = 8, activation = \"relu\"))\n",
    "mlpMod.add(Dense(12, activation = \"relu\"))\n",
    "mlpMod.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Compile Model\n",
    "mlpMod.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Fit Model to Data\n",
    "mlpMod.fit(X, Y, epochs = 150, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Evaluate Model\n",
    "scores = mlpMod.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (mlpMod.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_______________________\n",
    "\n",
    "## Evaluating The Performance of Deep Learning Models in Keras\n",
    "\n",
    "Next we'll explore various ways to evaluate the accuracy of your model.\n",
    "\n",
    "#### Automatic Holdout/Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Automatic Verification Dataset in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "## Random seed \n",
    "numpy.random.seed(206)\n",
    "\n",
    "## Re-use Pima Indians data, splitting into independent and dependent variables\n",
    "X = dbtsDF.loc[:, '_c0':'_c7'].values\n",
    "Y = dbtsDF.loc[:, '_c8'].values\n",
    "\n",
    "## Create model\n",
    "autoValMod = Sequential()\n",
    "autoValMod.add(Dense(12, input_dim = 8, activation = \"relu\"))\n",
    "autoValMod.add(Dense(8, activation = \"relu\"))\n",
    "autoValMod.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "## Compile model\n",
    "autoValMod.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "## Fit model on data\n",
    "autoValMod.fit(X, Y, validation_split = 0.33, epochs = 150, batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Holdout/Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Manual Validation Set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Set seed again\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "## Split into 67% for training data and 33% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.33, random_state = seed)\n",
    "\n",
    "## Define model hyperparameters\n",
    "manValMod = Sequential()\n",
    "manValMod.add(Dense(12, input_dim = 8, activation = \"relu\"))\n",
    "manValMod.add(Dense(8, activation = \"relu\"))\n",
    "manValMod.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "## Compile Model\n",
    "manValMod.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "## Fit Model\n",
    "manValMod.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 150, batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual _k_-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MLP for Pima Indians dataset with 10-fold cross validation\n",
    "## Note: StratifiedKFold attempts to balance the number of instances of each class in the folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "## Define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = seed)\n",
    "\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    ## Create model\n",
    "    kfMod = Sequential()\n",
    "    kfMod.add(Dense(12, input_dim = 8, activation = \"relu\"))\n",
    "    kfMod.add(Dense(8, activation = \"relu\"))\n",
    "    kfMod.add(Dense(1, activation = \"sigmoid\"))\n",
    "    \n",
    "    ## Compile model\n",
    "    kfMod.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    \n",
    "    ## Fit model\n",
    "    kfMod.fit(X[train], Y[train], epochs = 150, batch_size = 10, verbose = 0)\n",
    "    \n",
    "    ## Evaluate\n",
    "    scores = kfMod.evaluate(X[test], Y[test], verbose = 0)\n",
    "    \n",
    "    print(\"%s: %.2f%%\" % (kfMod.metrics_names[1], scores[1]*100))\n",
    "    \n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras Models with Scikit-Learn Functions\n",
    "\n",
    "But why?  Well, you can take advantage of k-fold cross validation (as seen above) and functions for hyperparameter optimization.\n",
    "\n",
    "#### _k_-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MLP for Pima Indians dataset with 10-fold cross validation via sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy\n",
    "\n",
    "## Define our function to create our model, which we'll then pass to KerasClassifier\n",
    "def createModel():\n",
    "    ## Define model structure\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim = 8, activation = \"relu\"))\n",
    "    model.add(Dense(8, activation = \"relu\"))\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    ## Compile\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "## Set seed\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "## Use the KerasClassifier wrapper and pass the createModel() function as the build function\n",
    "model = KerasClassifier(build_fn = createModel, epochs = 150, batch_size = 10, verbose = 0)\n",
    "\n",
    "## Evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = seed)\n",
    "results = cross_val_score(model, X, Y, cv = kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a simpler way to do cross validation - we don't have to manually calculate the average score.  \n",
    "\n",
    "#### Hyperparameter Optimization using Grid Search\n",
    "\n",
    "Another advantage is to use grid search to find the optimimal hyperparameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This will take a couple minutes, be patient ##\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## Define new create_model() function with optimizer and init variables and their defaults\n",
    "def create_model(optimizer = \"rmsprop\" , init = \"glorot_uniform\"):\n",
    "  ## create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(12, input_dim = 8 , kernel_initializer = init , activation=\"relu\"))\n",
    "  model.add(Dense(8, kernel_initializer = init, activation =\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer = init, activation = \"sigmoid\"))\n",
    "  ## Compile model\n",
    "  model.compile(loss = \"binary_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "  return model\n",
    "\n",
    "gridSearchMod = KerasClassifier(build_fn = create_model, verbose = 0)\n",
    "\n",
    "## Define grid search epochs, batch size and optimizer\n",
    "optimizers = [\"rmsprop\" , \"adam\"]\n",
    "inits = [\"glorot_uniform\", \"normal\", \"uniform\"]\n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "\n",
    "## Put these parameters into a python dictionary\n",
    "param_grid = dict(optimizer = optimizers, epochs = epochs, batch_size = batches, init = inits)\n",
    "\n",
    "## Pass the model and grid search parameters to the GridSearchCV class in scikit-learn, then fit it on the data\n",
    "grid = GridSearchCV(estimator = gridSearchMod, param_grid = param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "## Summarize results ##\n",
    "## Best score/hyperparameters\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "## Accuracy statistics\n",
    "means = grid_result.cv_results_[\"mean_test_score\"]\n",
    "stds = grid_result.cv_results_[\"std_test_score\"]\n",
    "params = grid_result.cv_results_[\"params\"]\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of these exercises was to show how wrapping Keras models can give you access to functions in `sklearn` that make your life easier as as data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 with DSX Spark",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
